{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing standard TD with Emphatic TD in a simple \"conveyor belt\" environment.\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "- Undiscounted case $\\gamma = 0$ for all non-terminal states.\n",
    "- Single feature with the same value for all states\n",
    "- Interest uniform for all non-terminal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from copy import copy\n",
    "from itertools import chain\n",
    "\n",
    "import algorithms, environments,features, policy\n",
    "from algorithms import EmphaticTD, TD\n",
    "from environments.misc import ConveyorBelt\n",
    "from features import Bias, Combination, Int2Unary, Wrapper\n",
    "from policy import RandomPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attribute logging class\n",
    "class AttributeLogger:\n",
    "    def __init__(self, obj, names):\n",
    "        self._obj = obj\n",
    "        self._names = [x for x in names]\n",
    "        self.dct = {name: list() for name in names}\n",
    "        \n",
    "    def log(self):\n",
    "        for name in self._names:\n",
    "            value = getattr(self._obj, name)\n",
    "            self.dct[name].append(copy(value))\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        return self.dct[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For calculating errors in this domain\n",
    "def mse(a, b):\n",
    "    return np.mean((a - b)**2)\n",
    "\n",
    "def geometric(val, n):\n",
    "    \"\"\"Geometric series: 1 + val + val^2 + ... + val^n\"\"\"\n",
    "    return (1-val**n)/(1-val)\n",
    "\n",
    "def triangle(n):\n",
    "    return n*(n+1)/2\n",
    "\n",
    "def pyramid(n):\n",
    "    return n*(n+1)*(n+2)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_episodes(num_episodes, env, policy):\n",
    "    return [make_episode(env, policy) for i in range(num_episodes)]\n",
    "    \n",
    "\n",
    "def make_episode(env, policy):\n",
    "    env.reset()\n",
    "    ret = []\n",
    "    while not env.is_terminal():\n",
    "        # Observe, take action, get next observation, and compute reward\n",
    "        s  = env.observe()\n",
    "        a  = policy(s)\n",
    "        r  = env.do(a)\n",
    "        sp = env.observe()\n",
    "\n",
    "        # Append step to episode trajectory\n",
    "        ret.append((s, a, r, sp))\n",
    "    return ret\n",
    "\n",
    "def apply_fa(episodes, phi_func):\n",
    "    \"\"\"Apply function approximation to a series of episodes.\"\"\"\n",
    "    ret = []\n",
    "    for episode in episodes:\n",
    "        tmp = []\n",
    "        for step in episode[:-1]:\n",
    "            s, a, r, sp = step\n",
    "            fvec   = phi_func(s)\n",
    "            fvec_p = phi_func(sp)\n",
    "            tmp.append((fvec, a, r, fvec_p))\n",
    "        \n",
    "        # Account for final step of the episode\n",
    "        s, a, r, sp = episode[-1]\n",
    "        fvec   = phi_func(s)\n",
    "        fvec_p = np.zeros(phi_func.length, dtype=np.float)\n",
    "        tmp.append((fvec, a, r, fvec_p))\n",
    "        ret.append(tmp)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some episodes\n",
    "random_seed = 101\n",
    "num_episodes = 1000\n",
    "num_states = 3\n",
    "\n",
    "# Setup environment\n",
    "env = ConveyorBelt(num_states, random_seed=random_seed)\n",
    "env.reward = lambda s, a, sp: 1\n",
    "\n",
    "# Set policy\n",
    "pol = RandomPolicy(env, random_seed=random_seed)\n",
    "\n",
    "# Get the raw episode data\n",
    "raw_episodes = make_episodes(num_episodes, env, pol)\n",
    "\n",
    "# Convert to features for function approximation\n",
    "phi = Wrapper(Bias(), terminals=env.terminals)\n",
    "\n",
    "# Apply function approximation\n",
    "episodes = apply_fa(raw_episodes, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3.0, 1: 2.0, 2: 1.0}\n"
     ]
    }
   ],
   "source": [
    "class ExpectedReturn:\n",
    "    \"\"\"Estimate expected returns (for tabular case).\"\"\"\n",
    "    def __init__(self, gamma=1.0):\n",
    "        self._gamma = gamma\n",
    "        \n",
    "    def __call__(self, episodes):\n",
    "        value  = defaultdict(list)\n",
    "        visits = defaultdict(int)\n",
    "        for episode in episodes:\n",
    "            ret = 0\n",
    "            for step in reversed(episode):\n",
    "                s, a, r, sp = step\n",
    "                if isinstance(s, np.ndarray):\n",
    "                    s = tuple(s)\n",
    "                elif hasattr(s, '__iter__'):\n",
    "                    s = tuple(s)\n",
    "                ret = ret*self._gamma + r\n",
    "                visits[s] += 1\n",
    "                value[s].append(ret)\n",
    "        return {s: sum(value[s])/visits[s] for s in value.keys()}\n",
    "    \n",
    "ER = ExpectedReturn()\n",
    "exp_ret = ER(raw_episodes)\n",
    "print(exp_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Value Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EveryVisitMC:\n",
    "    def __init__(self, n, gamma=1.0):\n",
    "        self.n = n\n",
    "        self._gamma = gamma\n",
    "        \n",
    "    def __call__(self, episodes):\n",
    "        value  = np.zeros(self.n)\n",
    "        visits = np.zeros(self.n)\n",
    "        for episode in episodes:\n",
    "            ret = 0\n",
    "            for step in reversed(episode):\n",
    "                s, a, r, sp = step\n",
    "                s = np.array(s)\n",
    "                ret = ret*self._gamma + r\n",
    "                visits += s\n",
    "                value += s*ret\n",
    "        return value/visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC = EveryVisitMC(phi.length, gamma=1.0)\n",
    "MC(episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.40357138]\n"
     ]
    }
   ],
   "source": [
    "# Specify the parameters for the episode\n",
    "fixed_params = {'alpha': 0.01, \n",
    "                'gamma': 1.0, \n",
    "                'lmbda': 0.5}\n",
    "\n",
    "# Parameters which are functions (e.g., of state)\n",
    "param_funcs = {}\n",
    "\n",
    "# Setup the agent\n",
    "agent = TD(phi.length)\n",
    "\n",
    "# Keep track of information over the episodes\n",
    "log_names = ['theta', 'z']\n",
    "logger = AttributeLogger(agent, log_names)\n",
    "\n",
    "# Perform learning\n",
    "for episode in episodes:\n",
    "    agent.reset()\n",
    "    for step in episode:\n",
    "        s, a, r, sp = step\n",
    "        \n",
    "        # Parameters for update\n",
    "        params = {}\n",
    "        for name, func in param_funcs.items():\n",
    "            params[k] = func(s)\n",
    "        params.update(**fixed_params)\n",
    "        \n",
    "        # Update the agent\n",
    "        agent.update(s, r, sp, params)\n",
    "        \n",
    "        # Record information\n",
    "        logger.log()\n",
    "        \n",
    "print(agent.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4285714285714284\n"
     ]
    }
   ],
   "source": [
    "# Calculation for the asymptopic TD value\n",
    "ns = 3\n",
    "lm = 0.5\n",
    "\n",
    "traces = [geometric(lm, n) for n in range(1, ns+1)]\n",
    "approx = sum(traces)/(traces[-1])\n",
    "print(approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emphatic TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.44]\n"
     ]
    }
   ],
   "source": [
    "# Specify the parameters for the episode\n",
    "fixed_params = {'alpha': 0.01, \n",
    "                'gamma': 1.0, \n",
    "                'lmbda': 0.0, \n",
    "                'interest': 1.0,}\n",
    "\n",
    "# Parameters which are functions (e.g., of state)\n",
    "param_funcs = {}\n",
    "\n",
    "# Setup the agent\n",
    "agent = EmphaticTD(phi.length)\n",
    "\n",
    "# Keep track of information over the episodes\n",
    "log_names = ['theta', 'z']\n",
    "logger = AttributeLogger(agent, log_names)\n",
    "\n",
    "# Perform learning\n",
    "for episode in episodes:\n",
    "    agent.reset()\n",
    "    for step in episode:\n",
    "        s, a, r, sp = step\n",
    "        \n",
    "        # Parameters for update\n",
    "        params = {}\n",
    "        for name, func in param_funcs.items():\n",
    "            params[k] = func(s)\n",
    "        params.update(**fixed_params)\n",
    "        \n",
    "        # Update the agent\n",
    "        agent.update(s, r, sp, params)\n",
    "        \n",
    "        # Record information\n",
    "        logger.log()\n",
    "        \n",
    "print(agent.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
