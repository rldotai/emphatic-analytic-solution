{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD with Emphasis\n",
    "\n",
    "According to the definitive source, the algorithm can be described (for accumulating traces) via:\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_t &= R_{t+1} + \\gamma_{t+1}\\theta_{t}^{\\top} \\phi_{t+1} - \\theta_{t}^{\\top}\\phi_{t} \n",
    "\\\\\n",
    "F_{t} &= \\rho_{t-1}\\gamma_{t} F_{t-1} + I_{t}\n",
    "\\\\\n",
    "M_{t} &= \\lambda_{t}I_{t} + (1 - \\lambda_{t})F_{t} \n",
    "\\\\\n",
    "e_{t} &= \\gamma_{t} \\lambda_{t} e_{t-1} + \\alpha_{t} M_{t} \\phi_{t}\n",
    "\\\\\n",
    "\\theta_{t+1} &= \\theta_{t} + \\delta_{t} e_{t}\n",
    "\\end{align}\n",
    "\n",
    "If we avoid parameters that vary per-timestep, and assume that $\\gamma = 0$ and $\\phi_{T} = \\vec{0}$ in the terminal state, we can write it more simply as \n",
    "\n",
    "\\begin{align}\n",
    "\\delta_t &= R_{t+1} + \\gamma \\theta_{t}^{\\top} \\phi_{t+1} - \\theta_{t}^{\\top}\\phi_{t} \n",
    "\\\\\n",
    "F_{t} &= \\rho_{t-1}\\gamma F_{t-1} + I_{t}\n",
    "\\\\\n",
    "M_{t} &= \\lambda I_{t} + (1 - \\lambda)F_{t} \n",
    "\\\\\n",
    "e_{t} &= \\gamma \\lambda e_{t-1} + M_{t} \\phi_{t}\n",
    "\\\\\n",
    "\\theta_{t+1} &= \\theta_{t} + \\alpha \\delta_{t} e_{t}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General\n",
    "\n",
    "## Particular Values\n",
    "\n",
    "For $\\gamma = 1$, $F_{t} = \\sum_{k=1}I_{k}$\n",
    "\n",
    "In this case, we have to start worrying about precision; a sufficiently long episode might take us well out of the realm where early states matter at all.\n",
    "\n",
    "## Least Squares Methods\n",
    "\n",
    "LSTD and ELSTD both get the feature weights via \n",
    "\n",
    "$$\\theta = A^{-1} b$$\n",
    "\n",
    "Where \n",
    "\n",
    "$$A_{t} = A_{0} + \\sum_{t=1}^{t}z_{t} (\\phi_{k} - \\gamma_{k+1} \\phi_{k+1})$$\n",
    "\n",
    "$$b_{t}  = b_0 + \\sum_{k=1}^{t} z_{k} R_{k+1}$$\n",
    "\n",
    "The main difference comes from how the traces are updated.\n",
    "\n",
    "\n",
    "## When Should We Expect Emphasis To Be Better?\n",
    "\n",
    "In the tabular case, both algorithms should perform about as well as each other, provided that the emphasis $M_{t}$ is nonzero during the episode (maybe more accurate to say so long as it's nonzero when $\\phi$ is nonzero).\n",
    "\n",
    "We can further say that there is no advantage to using emphasis when there is no state aliasing during the episodes under consideration, because this reduces to the tabular case as well.\n",
    "\n",
    "However, we *will* expect emphatic algorithms to perform better according to our interest-weighted error measure when there is state aliasing.\n",
    "The emphasis placed on states with more interest (and their successors for $\\lambda > 0$) will weight them more highly, making their approximation more accurate than their counterparts which might have a similar feature vector but less interest.\n",
    "\n",
    "## What should we set interest to be? (Undiscounted)\n",
    "\n",
    "In the undiscounted case, we probably want to estimate the reward of the start state most accurately.\n",
    "\n",
    "As such, having an interest that is nonzero at any other time presents a bit of an issue, because the followon trace is the sum of the interest up to that point in time, and $M_t$ varies linearly with $F_t$.\n",
    "\n",
    "What about having interest be allocated to *only* the start state?\n",
    "\n",
    "This too presents a problem, because trajectories that repeatedly visit the start state will be weighted more highly, and I am not sure about how to interpret that... it's not quite Every-Visit-MC nor is it First-Visit-MC.\n",
    "\n",
    "So it's probably best to have it equal to `1` in the start state of each episode, and `0` at all other times.\n",
    "\n",
    "## What should we set lambda to be?\n",
    "\n",
    "In the case where $\\lambda = 0$, we have essentially TD(0) for the undiscounted case with interest in the first state only.\n",
    "\n",
    "In the case where $\\lambda = 1$ w/ start-state interest, we are essentially performing First-Visit MC on the initial state only.\n",
    "\n",
    "In the case where $\\lambda = 1$ w/ first-visit to each state interest, we are performing the First-Visit MC version of TD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
